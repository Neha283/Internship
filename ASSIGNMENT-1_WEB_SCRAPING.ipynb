{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa077b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header Tags from Wikipedia Main Page:\n",
      "Main Page\n",
      "Welcome to Wikipedia\n",
      "From today's featured article\n",
      "Did you know ...\n",
      "In the news\n",
      "On this day\n",
      "Today's featured picture\n",
      "Other areas of Wikipedia\n",
      "Wikipedia's sister projects\n",
      "Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "#1) Write a python program to display all the header tags from wikipedia.org.\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "def header_tag(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        return header_tags\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage.\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "    header_tags = header_tag(url)\n",
    "    if header_tags:\n",
    "        print(\"Header Tags from Wikipedia Main Page:\")\n",
    "        for tag in header_tags:\n",
    "            print(tag.text.strip())\n",
    "    else:\n",
    "        print(\"No header tags found.\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7aeb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7591242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Restaurant Details from Dineout:\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Karol Bagh, Central Delhi', 'Ratings': '4.1', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'M-Block,Connaught Place, Central Delhi', 'Ratings': '4.3', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Connaught Place, Central Delhi', 'Ratings': '4', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Janpath, Central Delhi', 'Ratings': '4.4', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Connaught Place, Central Delhi', 'Ratings': '4.3', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Connaught Place, Central Delhi', 'Ratings': '4', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'F-Block,Connaught Place, Central Delhi', 'Ratings': '4', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Connaught Place, Central Delhi', 'Ratings': '4', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Regal Cinema Complex,Connaught Place, Central Delhi', 'Ratings': '4.4', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Connaught Place, Central Delhi', 'Ratings': '4.2', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Scindia House,Connaught Place, Central Delhi', 'Ratings': '4', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Connaught Place, Central Delhi', 'Ratings': '5', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'M-Block,Connaught Place, Central Delhi', 'Ratings': '4.3', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'M-Block,Connaught Place, Central Delhi', 'Ratings': '4.4', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Connaught Place, Central Delhi', 'Ratings': '3.8', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Janpath, Central Delhi', 'Ratings': '3.7', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Connaught Place, Central Delhi', 'Ratings': '5', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Connaught Place, Central Delhi', 'Ratings': '4.3', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Connaught Place, Central Delhi', 'Ratings': '4.1', 'Image URL': None}\n",
      "{'Name': None, 'Cuisine': None, 'Location': 'Connaught Place, Central Delhi', 'Ratings': '4.9', 'Image URL': None}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_dineout_details():\n",
    "    url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        restaurants = []\n",
    "        # Find all the restaurant cards\n",
    "        restaurant_cards = soup.find_all('div', class_='restnt-card restaurant')\n",
    "        for card in restaurant_cards:\n",
    "            # Initialize variables for each attribute\n",
    "            name = cuisine = location = ratings = image_url = None\n",
    "            \n",
    "            # Restaurant Name\n",
    "            name_tag = card.find('div', class_='restnt-info cursor').h4\n",
    "            if name_tag:\n",
    "                name = name_tag.text.strip()\n",
    "            \n",
    "            # Cuisine\n",
    "            cuisine_tag = card.find('div', class_='restnt-info cursor').p\n",
    "            if cuisine_tag:\n",
    "                cuisine = cuisine_tag.text.strip()\n",
    "            \n",
    "            # Location\n",
    "            location_tag = card.find('div', class_='restnt-loc ellipsis')\n",
    "            if location_tag:\n",
    "                location = location_tag.text.strip()\n",
    "            \n",
    "            # Ratings\n",
    "            ratings_tag = card.find('div', class_='restnt-rating')\n",
    "            if ratings_tag:\n",
    "                ratings = ratings_tag.text.strip()\n",
    "            \n",
    "            # Image URL\n",
    "            image_tag = card.find('div', class_='restnt-thumb')\n",
    "            if image_tag and 'data-img' in image_tag.attrs:\n",
    "                image_url = image_tag['data-img'].strip()\n",
    "            \n",
    "            # Append details to the restaurants list\n",
    "            restaurants.append({\n",
    "                'Name': name,\n",
    "                'Cuisine': cuisine,\n",
    "                'Location': location,\n",
    "                'Ratings': ratings,\n",
    "                'Image URL': image_url\n",
    "            })\n",
    "        \n",
    "        return restaurants\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage.\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    dineout_details = scrape_dineout_details()\n",
    "    if dineout_details:\n",
    "        print(\"Scraped Restaurant Details from Dineout:\")\n",
    "        for restaurant in dineout_details:\n",
    "            print(restaurant)\n",
    "    else:\n",
    "        print(\"No restaurant details scraped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad67b3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch the webpage.\n",
      "No data scraped.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_former_finance_ministers():\n",
    "    url = \"https://presidentofindia.nic.in/former-presidents.html\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        finance_ministers = []\n",
    "        # Find the table containing the former finance ministers\n",
    "        table = soup.find('table', class_='tablepresidents')\n",
    "        if table:\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows[1:]:  # Skip the header row\n",
    "                cols = row.find_all('td')\n",
    "                if len(cols) >= 3:  # Ensure the row has enough columns\n",
    "                    name = cols[0].text.strip()\n",
    "                    term_of_office = cols[1].text.strip()\n",
    "                    finance_ministers.append({'Name': name, 'Term of Office': term_of_office})\n",
    "        return finance_ministers\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage.\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    finance_ministers_data = scrape_former_finance_ministers()\n",
    "    if finance_ministers_data:\n",
    "        df = pd.DataFrame(finance_ministers_data)\n",
    "        print(\"List of Respected Former Finance Ministers of India:\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
