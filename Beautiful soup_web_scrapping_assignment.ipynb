{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a29ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Name  Rating  Year\n",
      "0                     Ship of Theseus     8.0  2012\n",
      "1                              Iruvar     8.4  1997\n",
      "2                     Kaagaz Ke Phool     7.8  1959\n",
      "3   Lagaan: Once Upon a Time in India     8.1  2001\n",
      "4                     Pather Panchali     8.2  1955\n",
      "..                                ...     ...   ...\n",
      "95                        Apur Sansar     8.4  1959\n",
      "96                        Kanchivaram     8.2  2008\n",
      "97                    Monsoon Wedding     7.3  2001\n",
      "98                              Black     8.1  2005\n",
      "99                            Deewaar     8.0  1975\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Write a python program to display IMDB’s Top rated 100 Indian movies’ datahttps://www.imdb.com/list/ls056092300/ (i.e. name, rating, year ofrelease) and make data frame.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.imdb.com/list/ls056092300/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "movie_items = soup.find_all('div', class_='lister-item-content')\n",
    "names = []\n",
    "ratings = []\n",
    "years = []\n",
    "for item in movie_items:\n",
    "    name = item.find('a').text.strip()\n",
    "    names.append(name)\n",
    "\n",
    "    rating = item.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "    ratings.append(float(rating))\n",
    "\n",
    "    year = item.find('span', class_='lister-item-year').text.strip('()')\n",
    "    years.append(year)\n",
    "\n",
    "df = pd.DataFrame({'Name': names,'Rating': ratings,'Year': years})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d976ed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Product Name, Price, Discount]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Write a python program to scrape product name, price and discounts from https://peachmode.com/search?q=bags\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the website\n",
    "url = 'https://peachmode.com/search?q=bags'\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "product_items = soup.find_all('div', class_='product-block')\n",
    "\n",
    "names = []\n",
    "prices = []\n",
    "discounts = []\n",
    "\n",
    "for item in product_items:\n",
    "    # Extract product name\n",
    "    name = item.find(class_='product-thumbnail').text.strip()\n",
    "    names.append(name)\n",
    "\n",
    "    # Extract product price\n",
    "    price = item.find('div', class_='price st-money done').find('span', class_='regular-price').text.strip()\n",
    "    prices.append(price)\n",
    "\n",
    "    # Extract product discount, if available\n",
    "    discount_tag = item.find('div', class_='discounted_price st-money done').find('span', class_='discount')\n",
    "    if discount_tag:\n",
    "        discount = discount_tag.text.strip()\n",
    "    else:\n",
    "        discount = 'No discount'\n",
    "    discounts.append(discount)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df = pd.DataFrame({\n",
    "    'Product Name': names,\n",
    "    'Price': prices,\n",
    "    'Discount': discounts\n",
    "})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c787734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "Empty DataFrame\n",
      "Columns: [Team, Matches, Points, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 ODI Batsmen:\n",
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 ODI Bowlers:\n",
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "#a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "#b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "#c) Top 10 ODI bowlers along with the records of their team and rating.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape top 10 ODI teams\n",
    "def scrape_teams():\n",
    "    url = 'https://www.icc-cricket.com/rankings/team-rankings/mens/odi'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "    for team in soup.select('.table-body .rankings-block__team-name'):\n",
    "        teams.append(team.text.strip())\n",
    "    for match in soup.select('.table-body .table-body__cell.u-center-text')[0::5]:\n",
    "        matches.append(match.text.strip())\n",
    "    for point in soup.select('.table-body .table-body__cell.u-center-text')[1::5]:\n",
    "        points.append(point.text.strip())\n",
    "    for rating in soup.select('.table-body .table-body__cell.u-text-right.rating'):\n",
    "        ratings.append(rating.text.strip())\n",
    "    return pd.DataFrame({\"Team\": teams[:10],\n",
    "        \"Matches\": matches[:10],\n",
    "        \"Points\": points[:10],\n",
    "        \"Rating\": ratings[:10]\n",
    "    })\n",
    "\n",
    "# Function to scrape top 10 ODI batsmen\n",
    "def scrape_batsmen():\n",
    "    url = 'https://www.icc-cricket.com/rankings/batting/mens/odi'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    for player in soup.select('.table-body .table-body__cell.name a'):\n",
    "        players.append(player.text.strip())\n",
    "    for team in soup.select('.table-body .table-body__cell.nationality-logo span'):\n",
    "        teams.append(team.text.strip())\n",
    "    for rating in soup.select('.table-body .table-body__cell.u-text-right.rating'):\n",
    "        ratings.append(rating.text.strip())\n",
    "    return pd.DataFrame({\n",
    "        \"Player\": players[:10],\n",
    "        \"Team\": teams[:10],\n",
    "        \"Rating\": ratings[:10]\n",
    "    })\n",
    "\n",
    "# Function to scrape top 10 ODI bowlers\n",
    "def scrape_bowlers():\n",
    "    url = 'https://www.icc-cricket.com/rankings/bowling/mens/odi'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    for player in soup.select('.table-body .table-body__cell.name a'):\n",
    "        players.append(player.text.strip())\n",
    "    for team in soup.select('.table-body .table-body__cell.nationality-logo span'):\n",
    "        teams.append(team.text.strip())\n",
    "    for rating in soup.select('.table-body .table-body__cell.u-text-right.rating'):\n",
    "        ratings.append(rating.text.strip())\n",
    "    return pd.DataFrame({\n",
    "        \"Player\": players[:10],\n",
    "        \"Team\": teams[:10],\n",
    "        \"Rating\": ratings[:10]\n",
    "    })\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Top 10 ODI Teams:\")\n",
    "    print(scrape_teams())\n",
    "    print(\"\\nTop 10 ODI Batsmen:\")\n",
    "    print(scrape_batsmen())\n",
    "    print(\"\\nTop 10 ODI Bowlers:\")\n",
    "    print(scrape_bowlers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc2a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the heading, date, content and the likes for the video from the link for the youtube video from the post.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the Patreon page\n",
    "url = 'https://www.patreon.com/coreyms'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the posts\n",
    "posts = soup.find_all('div', class_='sc-1n4z0lw-0 gndgBp')\n",
    "\n",
    "# Iterate over each post and extract details\n",
    "for post in posts:\n",
    "    # Extract heading\n",
    "    heading = post.find('h2', class_='sc-1hyy78r-0 fhljJo').text.strip()\n",
    "    \n",
    "    # Extract date\n",
    "    date = post.find('time')['datetime']\n",
    "    \n",
    "    # Extract content\n",
    "    content = post.find('div', class_='sc-1n4z0lw-1 kRnAvM').text.strip()\n",
    "    \n",
    "    # Extract likes for the embedded YouTube video (if available)\n",
    "    youtube_link = post.find('a', class_='sc-1svfimk-3 VbQxo')\n",
    "    if youtube_link:\n",
    "        youtube_url = youtube_link['href']\n",
    "        # Send a GET request to the YouTube URL\n",
    "        youtube_response = requests.get(youtube_url)\n",
    "        youtube_soup = BeautifulSoup(youtube_response.content, 'html.parser')\n",
    "        # Extract likes for the YouTube video\n",
    "        likes = youtube_soup.find('button', class_='style-scope yt-formatted-string').text.strip()\n",
    "    else:\n",
    "        likes = 'N/A'\n",
    "    \n",
    "    # Print the details\n",
    "    print(\"Heading:\", heading)\n",
    "    print(\"Date:\", date)\n",
    "    print(\"Content:\", content)\n",
    "    print(\"Likes:\", likes)\n",
    "    print(\"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d42a1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping details for Indira-nagar:\n",
      "\n",
      "\n",
      "\n",
      "Scraping details for Jayanagar:\n",
      "\n",
      "\n",
      "\n",
      "Scraping details for Rajaji-nagar:\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Write a python program to scrape house details from mentioned URL. It should include house title, location,\n",
    "#area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar,\n",
    "#Rajaji Nagar.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_house_details(locality):\n",
    "    url = f\"https://www.nobroker.in/property/sale/bangalore/{locality}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        house_listings = soup.find_all('div', class_='nb__2JHKO')\n",
    "        for listing in house_listings:\n",
    "            title = listing.find('h2', class_='heading-6').text.strip()\n",
    "            location = listing.find('div', class_='nb__2CMjv').text.strip()\n",
    "            area = listing.find('div', class_='nb__3oNyC').text.strip()\n",
    "            emi = listing.find('div', class_='font-semi-bold heading-6').text.strip()\n",
    "            price = listing.find('div', class_='heading-7').text.strip()\n",
    "            print(\"Title:\", title)\n",
    "            print(\"Location:\", location)\n",
    "            print(\"Area:\", area)\n",
    "            print(\"EMI:\", emi)\n",
    "            print(\"Price:\", price)\n",
    "            print(\"---------------------------\")\n",
    "    else:\n",
    "        print(\"Failed to fetch data from the website\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    localities = ['indira-nagar', 'jayanagar', 'rajaji-nagar']\n",
    "    for locality in localities:\n",
    "        print(f\"Scraping details for {locality.capitalize()}:\")\n",
    "        scrape_house_details(locality)\n",
    "        print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99839e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Product Details:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.bewakoof.com/bestseller?sort=popular\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Scraping product details\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m scrape_product_details(url)\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mscrape_product_details\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 Product Details:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, product \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(products[:\u001b[38;5;241m10\u001b[39m], start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 16\u001b[0m     product_name \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     17\u001b[0m     price \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscountedPrice\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     18\u001b[0m     image_url \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "#Write a python program to scrape first 10 product details which include product name , price , Image URL from\n",
    "#https://www.bewakoof.com/bestseller?sort=popular .\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_product_details(url):\n",
    "   # headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        products = soup.find_all('div', class_='productCardBox')\n",
    "\n",
    "        print(\"Top 10 Product Details:\")\n",
    "        for i, product in enumerate(products[:10], start=1):\n",
    "            product_name = product.find('div', class_='name').text.strip()\n",
    "            price = product.find('div', class_='discountedPrice').text.strip()\n",
    "            image_url = product.find('img')['src']\n",
    "\n",
    "            print(f\"\\nProduct {i}:\")\n",
    "            print(f\"Name: {product_name}\")\n",
    "            print(f\"Price: {price}\")\n",
    "            print(f\"Image URL: {image_url}\")\n",
    "    else:\n",
    "        print(\"Failed to fetch product details.\")\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "\n",
    "# Scraping product details\n",
    "scrape_product_details(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dec3e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Data:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.cnbc.com/world/?region=world\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Scraping headings, date, and news links\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m scrape_cnbc_news(url)\n",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m, in \u001b[0;36mscrape_cnbc_news\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     20\u001b[0m heading \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Extracting date\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m date \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Extracting news link\u001b[39;00m\n\u001b[0;32m     26\u001b[0m news_link \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "#Please visit https://www.cnbc.com/world/?region=world and scrap\u0002a) headings\n",
    "#b) date\n",
    "#c) News link\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_cnbc_news(url):\n",
    "    # Sending a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Finding all news articles\n",
    "        articles = soup.find_all('div', class_='Card-titleContainer')\n",
    "        \n",
    "        print(\"Scraped Data:\")\n",
    "        for article in articles:\n",
    "            # Extracting heading\n",
    "            heading = article.find('a').text.strip()\n",
    "            \n",
    "            # Extracting date\n",
    "            date = article.find('time').text.strip()\n",
    "            \n",
    "            # Extracting news link\n",
    "            news_link = article.find('a')['href']\n",
    "            \n",
    "            print(f\"Heading: {heading}\")\n",
    "            print(f\"Date: {date}\")\n",
    "            print(f\"News Link: {news_link}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Failed to fetch data.\")\n",
    "\n",
    "# URL of the CNBC website\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "# Scraping headings, date, and news links\n",
    "scrape_cnbc_news(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd3bbdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Data:\n"
     ]
    }
   ],
   "source": [
    "#)  8) Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded\u0002articles/ and scrap-\n",
    "# a) Paper title\n",
    "# b) date\n",
    "#c) Autho\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    # Sending a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Finding all article entries\n",
    "        articles = soup.find_all('div', class_='panel-item')\n",
    "        \n",
    "        print(\"Scraped Data:\")\n",
    "        for article in articles:\n",
    "            # Extracting paper title\n",
    "            title = article.find('h2', class_='panel-item-title').text.strip()\n",
    "\n",
    "            # Extracting date\n",
    "            date = article.find('span', class_='published-date').text.strip()\n",
    "\n",
    "            # Extracting author(s)\n",
    "            author = article.find('span', class_='author').text.strip()\n",
    "\n",
    "            print(f\"Paper Title: {title}\")\n",
    "            print(f\"Date: {date}\")\n",
    "            print(f\"Author: {author}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Failed to fetch data.\")\n",
    "\n",
    "# URL of the Most Downloaded Articles page\n",
    "url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/'\n",
    "\n",
    "# Scraping paper title, date, and author\n",
    "scrape_most_downloaded_articles(url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
